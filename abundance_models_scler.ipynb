{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will...\n",
    "   - calculate baseline error\n",
    "   - run random forest regressors on abundance \n",
    "   - load regional dfs add island col as ID, rbind all 5 dfs \n",
    "   - predict out for each group \n",
    "\n",
    "#### Scoring models\n",
    "\n",
    "We evaluated the performance of the abundance models based on mean average error (accuracy) and mean absolute percent error (MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, auc\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import *\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import utm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_types=['scler','branching','columnar','encrusting','free_livin','massive','plate']\n",
    "target_types=['binary','percent']\n",
    "df = dict()\n",
    "\n",
    "for i in range(0,len(coral_types)):\n",
    "    for j in range(0,len(target_types)):\n",
    "        df[str(coral_types[i])+'_'+str(target_types[j])]=pd.read_csv(str(coral_types[i])+'_'+str(target_types[j])+'.csv')\n",
    "        del df[str(coral_types[i])+'_'+str(target_types[j])]['Unnamed: 0'] # artifact indexing column\n",
    "# Access the data as, e.g., df['scler_percent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scler (all corals)\n",
    "\n",
    "### Train/test split, baseline error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is the estimate I would get if I simply predicted the average abundance across all cells. If I can improve upon this by using my model, then my approach is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scler Training Features Shape: (1372, 9)\n",
      "Scler Training Labels Shape: (1372,)\n",
      "Scler Testing Features Shape: (588, 9)\n",
      "Scler Testing Labels Shape: (588,)\n",
      "Baseline prediction error:  14.31\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['scler_percent'].drop(['Sclr_Rw','lat','lon','ID'], axis=1), \n",
    "                                                    df['scler_percent']['Sclr_Rw'], \n",
    "                                                    test_size = 0.3, random_state = 30)\n",
    "\n",
    "\n",
    "print('Scler Training Features Shape:', X_train.shape)\n",
    "print('Scler Training Labels Shape:', y_train.shape)\n",
    "print('Scler Testing Features Shape:', X_test.shape)\n",
    "print('Scler Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "# The baseline predictions are the averages\n",
    "\n",
    "baseline_preds = np.array([y_train.mean()] * len(y_train))\n",
    "baseline_errors = abs(baseline_preds - y_train)\n",
    "print('Baseline prediction error: ', round(np.mean(baseline_errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  10.22\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error: ', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'max_features': 'sqrt', 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "bootstrap = [True, False] # Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 10 fold cross validation, \n",
    "# Search across n_iter * cv different combinations, and use all available cores\n",
    "rf_best = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 40, cv = 10, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_best.fit(X_train, y_train)\n",
    "print(rf_best.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  10.15\n"
     ]
    }
   ],
   "source": [
    "#Predicting on the test set\n",
    "predictions = rf_best.predict(X_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error: ', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  10.1\n",
      "[('bty', 0.17), ('rug', 0.14), ('slope', 0.13), ('shore_ed', 0.13), ('vill_ed', 0.13), ('asp', 0.12), ('hs', 0.08), ('ucur', 0.06), ('vcur', 0.05)]\n"
     ]
    }
   ],
   "source": [
    "# Retrain with the optimal parameters\n",
    "rf = RandomForestRegressor(n_estimators = 600, max_features = 'sqrt', bootstrap = True, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error: ', round(np.mean(errors), 2))\n",
    "\n",
    "# Get numerical feature importances\n",
    "feature_list = list(X_train.columns)\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "print(feature_importances)\n",
    "\n",
    "# Save the test data (actual) and predictions to df for visualizations later\n",
    "scler_preds = pd.DataFrame(data = {'actual': y_test, 'prediction': predictions})\n",
    "scler_preds.to_csv('scler_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the random forest regressor to predict percent abundance of all scleractinian corals across all sample sites reduced predictive error by nearly one-third (from 14.31% to 10.22%) compared to the application of a \"baseline average\" for prediction (i.e., the calculation of a uniform basic average abundance of coral across the sample sites). We used 10-fold cross validation over 40 iterations to tune the models, which further reduced our error by X% - Y% for each model (insert table reference here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('scler_abundance_model.pkl', 'wb') as fid:\n",
    "#    pickle.dump(rf, fid, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load regional dfs\n",
    "\n",
    "Adding an island ID column, concatenating the dfs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data/Tau')\n",
    "tau_bty_pts = pd.read_csv('tau_bty_pts.csv')\n",
    "tau_bty_pts.rename(columns={'tau_dbmb_mos': 'bty'}, inplace=True)\n",
    "tau_bty_pts['ID'] = 'Tau'\n",
    "\n",
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data/Tut')\n",
    "tut_bty_pts = pd.read_csv('tut_bty_pts.csv')\n",
    "tut_bty_pts.rename(columns={'tut_dbmb': 'bty'}, inplace=True)\n",
    "tut_bty_pts['ID'] = 'Tut'\n",
    "\n",
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data/OfuOlo')\n",
    "oo_bty_pts = pd.read_csv('oo_bty_pts.csv')\n",
    "oo_bty_pts.rename(columns={'depth': 'bty'}, inplace=True)\n",
    "oo_bty_pts['ID'] = 'OfuOlo'\n",
    "\n",
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data/Rose')\n",
    "rose_bty_pts = pd.read_csv('rose_bty_pts.csv')\n",
    "rose_bty_pts.rename(columns={'rose_5m_dbmb': 'bty'}, inplace=True)\n",
    "rose_bty_pts['ID'] = 'Rose'\n",
    "\n",
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data/Swains')\n",
    "swains_bty_pts = pd.read_csv('swains_bty_pts.csv')\n",
    "swains_bty_pts.rename(columns={'swa_dbmb_5m': 'bty'}, inplace=True)\n",
    "swains_bty_pts['ID'] = 'Swains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146074, 16)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data/')\n",
    "AS_all = pd.concat([tau_bty_pts, tut_bty_pts, oo_bty_pts,\n",
    "                   rose_bty_pts, swains_bty_pts])\n",
    "\n",
    "del AS_all['Unnamed: 0']\n",
    "del AS_all['dummy']\n",
    "print(AS_all.shape)\n",
    "AS_all.columns\n",
    "AS_all.bty = AS_all.bty * -1\n",
    "\n",
    "AS_all.head()\n",
    "AS_all.to_csv('AS_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict out to American Samoa concatenated df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144990, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bty</th>\n",
       "      <th>hs</th>\n",
       "      <th>rug</th>\n",
       "      <th>slope</th>\n",
       "      <th>asp</th>\n",
       "      <th>shore_ed</th>\n",
       "      <th>wl_ed</th>\n",
       "      <th>vill_third_q</th>\n",
       "      <th>vill_ed</th>\n",
       "      <th>esi</th>\n",
       "      <th>esi_ed</th>\n",
       "      <th>ucur</th>\n",
       "      <th>vcur</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>197.610001</td>\n",
       "      <td>1.959184</td>\n",
       "      <td>1.843792</td>\n",
       "      <td>40.829605</td>\n",
       "      <td>259.345642</td>\n",
       "      <td>784.577155</td>\n",
       "      <td>6325.764809</td>\n",
       "      <td>287.994</td>\n",
       "      <td>2487.227411</td>\n",
       "      <td>Freshwater Marshes</td>\n",
       "      <td>18418.813311</td>\n",
       "      <td>-0.00489</td>\n",
       "      <td>-0.01047</td>\n",
       "      <td>666925.0</td>\n",
       "      <td>8429050.0</td>\n",
       "      <td>Tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>198.049805</td>\n",
       "      <td>1.979381</td>\n",
       "      <td>1.600071</td>\n",
       "      <td>40.400249</td>\n",
       "      <td>144.983948</td>\n",
       "      <td>790.916596</td>\n",
       "      <td>6303.516019</td>\n",
       "      <td>287.994</td>\n",
       "      <td>2460.637660</td>\n",
       "      <td>Freshwater Marshes</td>\n",
       "      <td>18468.167397</td>\n",
       "      <td>-0.00489</td>\n",
       "      <td>-0.01047</td>\n",
       "      <td>666975.0</td>\n",
       "      <td>8429050.0</td>\n",
       "      <td>Tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>198.599426</td>\n",
       "      <td>1.989796</td>\n",
       "      <td>1.541323</td>\n",
       "      <td>43.999477</td>\n",
       "      <td>198.824860</td>\n",
       "      <td>799.507074</td>\n",
       "      <td>6281.586422</td>\n",
       "      <td>287.994</td>\n",
       "      <td>2434.784425</td>\n",
       "      <td>Freshwater Marshes</td>\n",
       "      <td>18517.524949</td>\n",
       "      <td>-0.00489</td>\n",
       "      <td>-0.01047</td>\n",
       "      <td>667025.0</td>\n",
       "      <td>8429050.0</td>\n",
       "      <td>Tau</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bty        hs       rug      slope         asp    shore_ed  \\\n",
       "674  197.610001  1.959184  1.843792  40.829605  259.345642  784.577155   \n",
       "675  198.049805  1.979381  1.600071  40.400249  144.983948  790.916596   \n",
       "676  198.599426  1.989796  1.541323  43.999477  198.824860  799.507074   \n",
       "\n",
       "           wl_ed  vill_third_q      vill_ed                 esi        esi_ed  \\\n",
       "674  6325.764809       287.994  2487.227411  Freshwater Marshes  18418.813311   \n",
       "675  6303.516019       287.994  2460.637660  Freshwater Marshes  18468.167397   \n",
       "676  6281.586422       287.994  2434.784425  Freshwater Marshes  18517.524949   \n",
       "\n",
       "        ucur     vcur         x          y   ID  \n",
       "674 -0.00489 -0.01047  666925.0  8429050.0  Tau  \n",
       "675 -0.00489 -0.01047  666975.0  8429050.0  Tau  \n",
       "676 -0.00489 -0.01047  667025.0  8429050.0  Tau  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AS_all = pd.read_csv('AS_all.csv')\n",
    "AS_all.dropna(inplace=True)\n",
    "print(AS_all.shape)\n",
    "AS_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS_all.to_csv('AS_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scler: bty, hs, rug, slope, asp, shore_ed, vill_ed, ucur, vcur\n",
    "scler_pred_out = AS_all[['bty', 'hs', 'rug', 'slope', 'asp', 'shore_ed', 'vill_ed', 'ucur', 'vcur']]\n",
    "scler_predictions = rf.predict(scler_pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.922521865889212\n",
      "11.22077150234499\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean())\n",
    "print(scler_predictions.mean())\n",
    "AS_all['scler_preds'] = scler_predictions\n",
    "AS_all.to_csv('AS_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS_all = pd.read_csv('AS_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144990, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AS_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bty', 'hs', 'rug', 'slope', 'asp', 'shore_ed', 'wl_ed', 'vill_third_q',\n",
       "       'vill_ed', 'esi', 'esi_ed', 'ucur', 'vcur', 'x', 'y', 'ID',\n",
       "       'scler_preds', 'plate_preds', 'massive_preds', 'free_living_preds',\n",
       "       'encrusting_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AS_all.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
