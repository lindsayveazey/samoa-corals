{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will...\n",
    "   - calculate baseline error\n",
    "   - run random forest regressors on abundance \n",
    "   - load regional dfs, impute, add island col as ID, rbind all 5 dfs \n",
    "   - predict out for each group \n",
    "\n",
    "#### Scoring models\n",
    "\n",
    "We evaluated the performance of the abundance models based on mean average error (accuracy) and mean absolute percent error (MAPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, auc\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import *\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_types=['scler','branching','columnar','encrusting','free_livin','massive','plate']\n",
    "target_types=['binary','percent']\n",
    "df = dict()\n",
    "\n",
    "for i in range(0,len(coral_types)):\n",
    "    for j in range(0,len(target_types)):\n",
    "        df[str(coral_types[i])+'_'+str(target_types[j])]=pd.read_csv(str(coral_types[i])+'_'+str(target_types[j])+'.csv')\n",
    "        del df[str(coral_types[i])+'_'+str(target_types[j])]['Unnamed: 0'] # artifact indexing column\n",
    "# Access the data as, e.g., df['scler_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split, basic GLM\n",
    "\n",
    "##### Scler (all corals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scler Training Features Shape: (1372, 9)\n",
      "Scler Training Labels Shape: (1372,)\n",
      "Scler Testing Features Shape: (588, 9)\n",
      "Scler Testing Labels Shape: (588,)\n",
      "Average baseline error:  0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       480\n",
      "           1       0.00      0.00      0.00       108\n",
      "\n",
      "    accuracy                           0.81       588\n",
      "   macro avg       0.41      0.50      0.45       588\n",
      "weighted avg       0.67      0.81      0.73       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['scler_binary'].drop(['Sclr_Cr','lat','lon','ID'], axis=1), \n",
    "                                                    df['scler_binary']['Sclr_Cr'], \n",
    "                                                    test_size = 0.3, random_state = 30)\n",
    "\n",
    "\n",
    "print('Scler Training Features Shape:', X_train.shape)\n",
    "print('Scler Training Labels Shape:', y_train.shape)\n",
    "print('Scler Testing Features Shape:', X_test.shape)\n",
    "print('Scler Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "# Fit the GLM\n",
    "logmodel.fit(X_train, y_train)\n",
    "predictions=logmodel.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our all-coral scleractinian grouping dataset to test the efficacy of applying a basic generalized linear model (i.e., a logit model, or GLM) to predict coral occurrence. The application of a basic GLM to these data resulted in predictions that substantially underestimated the occurrence of corals (e.g., the average occurrence in the original training data = 20%; in the predicted data, occurrence = <1%). In light of these poor results, we proceeded with the parameterization of a random forest classifier.\n",
    "\n",
    "## Train random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       480\n",
      "           1       0.62      0.31      0.41       108\n",
      "\n",
      "    accuracy                           0.84       588\n",
      "   macro avg       0.74      0.63      0.66       588\n",
      "weighted avg       0.82      0.84      0.82       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Check precision, F1 score\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random forest binary classifier improved upon the poor performance of the GLM (precision = 0.74 and F1 score = 0.66). We attempted to improve on these scores by adjusting the model to function as a regressor (i.e., produce predictions within the 0-1 range, versus binary 0 or 1 predictions). We interpreted the results based on varying thresholds that simultaneously maximized both precision and F1 scores, but were unable to improve upon the performance of the classifier based on our two chosen metrics.\n",
    "\n",
    "## Train random forest regressor on binary data, interpret results based on a selected threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       480\n",
      "           1       0.59      0.28      0.38       108\n",
      "\n",
      "    accuracy                           0.83       588\n",
      "   macro avg       0.72      0.62      0.64       588\n",
      "weighted avg       0.81      0.83      0.81       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Interpret results based on a threshold- trial and error\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] < 0.52:\n",
    "        predictions[i] = 0\n",
    "    else:\n",
    "        predictions[i] = 1\n",
    "        \n",
    "# Check precision, F1 score\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Scores to beat: precision = 0.74, f1-score = 0.66\n",
    "# ...not able to beat this ^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
