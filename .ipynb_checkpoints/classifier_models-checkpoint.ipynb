{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will...\n",
    "   - [X] run a basic GLM \n",
    "   - [X] run random forest classifier on binary scler data\n",
    "   - [X] evaluate performance based on key metrics\n",
    "   - load regional dfs, impute, add island col as ID, rbind all 5 dfs \n",
    "   - predict out for each group \n",
    "\n",
    "#### Scoring models\n",
    "\n",
    "To score the classifier models, we used the metric of precision, which evaluates performance based on the proportion of predicted positives that are truly positive. The model is penalized for false positives, which improves our confidence that the the regions of the map that are identified as likely to host corals do, in fact, host corals. We also scoring the models using the F1 score, which balances our priorities of correctly identifying locations of corals (precision) and finding as many corals in unsurveyed regions as possible (recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, auc\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import *\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_types=['scler','branching','columnar','encrusting','free_livin','massive','plate']\n",
    "target_types=['binary','percent']\n",
    "df = dict()\n",
    "\n",
    "for i in range(0,len(coral_types)):\n",
    "    for j in range(0,len(target_types)):\n",
    "        df[str(coral_types[i])+'_'+str(target_types[j])]=pd.read_csv(str(coral_types[i])+'_'+str(target_types[j])+'.csv')\n",
    "        del df[str(coral_types[i])+'_'+str(target_types[j])]['Unnamed: 0'] # artifact indexing column\n",
    "# Access the data as, e.g., df['scler_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split, basic GLM\n",
    "\n",
    "##### Scler (all corals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scler Training Features Shape: (1372, 9)\n",
      "Scler Training Labels Shape: (1372,)\n",
      "Scler Testing Features Shape: (588, 9)\n",
      "Scler Testing Labels Shape: (588,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       480\n",
      "           1       0.00      0.00      0.00       108\n",
      "\n",
      "    accuracy                           0.81       588\n",
      "   macro avg       0.41      0.50      0.45       588\n",
      "weighted avg       0.67      0.81      0.73       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['scler_binary'].drop(['Sclr_Cr','lat','lon','ID'], axis=1), \n",
    "                                                    df['scler_binary']['Sclr_Cr'], \n",
    "                                                    test_size = 0.3, random_state = 30)\n",
    "\n",
    "\n",
    "print('Scler Training Features Shape:', X_train.shape)\n",
    "print('Scler Training Labels Shape:', y_train.shape)\n",
    "print('Scler Testing Features Shape:', X_test.shape)\n",
    "print('Scler Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "# Fit the GLM\n",
    "logmodel.fit(X_train, y_train)\n",
    "predictions=logmodel.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our all-coral scleractinian grouping dataset to test the efficacy of applying a basic generalized linear model (i.e., a logit model, or GLM) to predict coral occurrence. The application of a basic GLM to these data resulted in predictions that substantially underestimated the occurrence of corals (e.g., the average occurrence in the original training data = 20%; in the predicted data, occurrence = <1%). In light of these poor results, we proceeded with the parameterization of a random forest classifier.\n",
    "\n",
    "## Train random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       480\n",
      "           1       0.62      0.31      0.41       108\n",
      "\n",
      "    accuracy                           0.84       588\n",
      "   macro avg       0.74      0.63      0.66       588\n",
      "weighted avg       0.82      0.84      0.82       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Check precision, F1 score\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to file\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "test_df = pd.merge(y_test_df, df['scler_binary'], right_index=True, left_index=True, on='Sclr_Cr') # merge based on index\n",
    "# Reset for ease of merge\n",
    "test_df = pd.DataFrame.reset_index(test_df)\n",
    "del test_df['index']\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.columns = ['prediction']\n",
    "\n",
    "rf_test_df = pd.merge(test_df, pred_df, right_index=True, left_index=True) # merge based on index\n",
    "rf_test_df.head()\n",
    "pd.DataFrame.to_csv(rf_test_df, 'scler_rf_test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the 'scler' classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8384353741496599\n"
     ]
    }
   ],
   "source": [
    "with open('scler_classifier_model.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid, 2)  \n",
    "\n",
    "# Load the model from disk\n",
    "loaded_model = pickle.load(open('scler_classifier_model.pkl', 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result) # This is a decent r^2 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random forest binary classifier improved upon the poor performance of the GLM (precision = 0.74 and F1 score = 0.66). We attempted to improve on these scores by adjusting the model to function as a regressor (i.e., produce predictions within the 0-1 range, versus binary 0 or 1 predictions). \n",
    "\n",
    "## Train random forest regressor on binary data, interpret results based on a selected threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       480\n",
      "           1       0.59      0.28      0.38       108\n",
      "\n",
      "    accuracy                           0.83       588\n",
      "   macro avg       0.72      0.62      0.64       588\n",
      "weighted avg       0.81      0.83      0.81       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Interpret results based on a threshold- trial and error\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] < 0.52:\n",
    "        predictions[i] = 0\n",
    "    else:\n",
    "        predictions[i] = 1\n",
    "        \n",
    "# Check precision, F1 score\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Scores to beat: precision = 0.74, f1-score = 0.66\n",
    "# ...not able to beat this ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpreted the results based on varying thresholds that simultaneously maximized both precision and F1 scores, which resulted in superior model performance, particularly for those coral groups in in which the rare occurrence of corals proved too challenging for the classifier to handle.\n",
    "\n",
    "## Train random forest classifiers for remaining coral groups\n",
    "\n",
    "##### Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scler Training Features Shape: (2214, 9)\n",
      "Scler Training Labels Shape: (2214,)\n",
      "Scler Testing Features Shape: (949, 9)\n",
      "Scler Testing Labels Shape: (949,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1000, random_state=30)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['branching_binary'].drop(['Brnch_C','lat','lon','ID'], axis=1), \n",
    "                                                    df['branching_binary']['Brnch_C'], \n",
    "                                                    test_size = 0.3, random_state = 30)\n",
    "\n",
    "\n",
    "print('Scler Training Features Shape:', X_train.shape)\n",
    "print('Scler Training Labels Shape:', y_train.shape)\n",
    "print('Scler Testing Features Shape:', X_test.shape)\n",
    "print('Scler Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 30)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       927\n",
      "           1       0.25      0.18      0.21        22\n",
      "\n",
      "    accuracy                           0.97       949\n",
      "   macro avg       0.62      0.58      0.60       949\n",
      "weighted avg       0.96      0.97      0.97       949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] < 0.33:\n",
    "        predictions[i] = 0\n",
    "    else:\n",
    "        predictions[i] = 1\n",
    "        \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# 0.1 0.52 0.53\n",
    "# 0.2 0.57 0.57\n",
    "# 0.3 0.6 0.59\n",
    "# 0.31 0.61 0.59\n",
    "# 0.33 0.62 0.6\n",
    "# 0.34 0.6 0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07894129219378287\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "test_df = pd.merge(y_test_df, df['branching_binary'], right_index=True, left_index=True, on='Brnch_C') \n",
    "test_df = pd.DataFrame.reset_index(test_df)\n",
    "del test_df['index']\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.columns = ['prediction']\n",
    "\n",
    "rf_test_df = pd.merge(test_df, pred_df, right_index=True, left_index=True)\n",
    "rf_test_df.head()\n",
    "pd.DataFrame.to_csv(rf_test_df, 'branching_rf_test_df.csv')\n",
    "\n",
    "with open('branching_classifier_model.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid, 2)  \n",
    "\n",
    "# Load the model from disk\n",
    "loaded_model = pickle.load(open('branching_classifier_model.pkl', 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result) # This is odd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columnar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encrusting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Free living"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plate-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
