{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will...\n",
    "   - calculate baseline error\n",
    "   - run random forest regressors on abundance \n",
    "   - load regional dfs, impute, add island col as ID, rbind all 5 dfs \n",
    "   - predict out for each group \n",
    "\n",
    "#### Scoring models\n",
    "\n",
    "We evaluated the performance of the abundance models based on mean average error (accuracy) and mean absolute percent error (MAPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, auc\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import *\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/linds/OneDrive/Documents/samoa_corals_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_types=['columnar']\n",
    "target_types=['binary','percent']\n",
    "df = dict()\n",
    "\n",
    "for i in range(0,len(coral_types)):\n",
    "    for j in range(0,len(target_types)):\n",
    "        df[str(coral_types[i])+'_'+str(target_types[j])]=pd.read_csv(str(coral_types[i])+'_'+str(target_types[j])+'.csv')\n",
    "        del df[str(coral_types[i])+'_'+str(target_types[j])]['Unnamed: 0'] # artifact indexing column\n",
    "# Access the data as, e.g., df['scler_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnar Training Features Shape: (2214, 9)\n",
      "Columnar Training Labels Shape: (2214,)\n",
      "Columnar Testing Features Shape: (949, 9)\n",
      "Columnar Testing Labels Shape: (949,)\n",
      "Baseline prediction error:  0.11\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['columnar_percent'].drop(['Clmnr_R','lat','lon','ID'], axis=1), \n",
    "                                                    df['columnar_percent']['Clmnr_R'], \n",
    "                                                    test_size = 0.3, random_state = 30)\n",
    "\n",
    "\n",
    "print('Columnar Training Features Shape:', X_train.shape)\n",
    "print('Columnar Training Labels Shape:', y_train.shape)\n",
    "print('Columnar Testing Features Shape:', X_test.shape)\n",
    "print('Columnar Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "# The baseline predictions are the averages\n",
    "\n",
    "baseline_preds = np.array([y_train.mean()] * len(y_train))\n",
    "baseline_errors = abs(baseline_preds - y_train)\n",
    "print('Baseline prediction error: ', round(np.mean(baseline_errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  0.17\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 30)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting to the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error: ', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3153\n",
       "20       9\n",
       "40       1\n",
       "Name: Clmnr_R, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['columnar_percent'].Clmnr_R.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "bootstrap = [True, False] # Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 10 fold cross validation, \n",
    "# Search across n_iter * cv different combinations, and use all available cores\n",
    "rf_best = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 40, cv = 10, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_best.fit(X_train, y_train)\n",
    "print(rf_best.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on the test set\n",
    "predictions = rf_best.predict(X_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error: ', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columnar_abundance_model.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
